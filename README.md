# SVScope
Structural variation detection for short read via multi-source fusion and multi-channel visual filtering.

## Introduction

Structural variations (SVs) are a major source of genomic diversity and are closely associated with human disease. Existing short-read-based SV detection tools often rely on limited alignment features, which restricts their ability to fully capture variation signals. While many multi-source fusion methods have improved recall rates, they also introduce a large number of false positives. To address these issues, we present SVScope, an integrated SV detection tool that fuses multi-source signals, structure-sensitive image encoding, and deep visual filtering. It first consolidates candidate variations derived from complementary alignment signals into a standardized candidate set via a harmonized detection and merging pipeline. To further improve specificity, SVScope encodes alignment features of candidate regions into multi-channel image representations and employs a convolutional neural network with embedded attention mechanisms to filter out false-positive calls. Benchmarking on a real dataset demonstrates that SVScope consistently improves recall compared to individual detection tools while substantially enhancing overall precision through deep learning-based filtering. These results highlight SVScope’s capacity to balance sensitivity and specificity, offering a scalable and robust solution for SV analysis in large-scale short-read sequencing studies.

## 1. SVScope channel configuration:

```
conda config --add channels - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
conda config --add channels - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
conda config --add channels defaults
conda config --add channels conda-forge
conda config --add channels bioconda
```

## 2. SVScope installation script:

```
./SVScope_install.sh
```

## 3. Reload .bashrc:

```
source ~/.bashrc
```

## 4. Activate the SVScope environment:

```
conda activate SVScope
```

## 5. Edit the scripts for the three algorithms into executable mode:

```
chmod +x Algorithm*.sh
```

## 6. Run SVScope_detect.sh to obtain the VCF file generated by SVScope but not filtered:

```
bash SVScope_detect.sh \
  --bam_path /your_path/your_bam_name.bam  \
  --referenceFasta /your_path/your_fa_name.fa \
  --out_dir /your_output_dir \
  --prefix /your_sample_name (e.g. HG002) \
  --jobs thread_num(e.g. 4)
```

##### [Note: The three SV detection algorithms can also be called separately. If you do not want to use the multi-source integration command provided by SVScope, bash ./SVScope_detect.sh, you can also call the following three tools separately. All commands have been encapsulated, and you can call them directly by passing external parameters. The specific usage is as follows:]

##### For Algorithm1, the call command is:

```
bash Algorithm1.sh --bam_path /your_path/your_bam_name.bam 
                   --referenceFasta /your_path/your_fa_name.fa
                   --out_dir /your_output_dir
```

##### For Algorithm2, the call command is:

```
bash Algorithm2.sh --bam_path /your_path/your_bam_name.bam 
		   --out_dir /your_output_dir
		   --referenceFasta /your_path/your_fa_name.fa 
                   --jobs thread_num(e.g. 4)
```

##### For Algorithm3, the call command is:

```
bash Algorithm3.sh --bam_path /your_path/your_bam_name.bam 
                   --out_dir /your_output_dir
                   --prefix /your_sample_name (e.g. HG002)
```



## 7.SVScope then performs image encoding and filtering on the detected VCF files：

##### [Note: Since filtering and detection have different requirements for the Python version, we need to create a new Conda environment for deep learning filtering, named SVScope_filter. The one-click installation steps are as follows:]

## Installation

```
conda env create -f SVScope_environment.yml
conda activate SVScope_filter
```

### Requirements

- Python 3.6                                                           

- Pytorch1.10.2

- Pysam 0.15.4

- numpy 1.19.5

- scikit-learn 0.24.2

- torchvision 0.11.3 

- tensorboard 2.8.0

- cudatookit 11.3.1

## Datasets

#### Reference

- hs37d5: https://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz
- hg19:http://hgdownload.cse.ucsc.edu/goldenPath/hg19/bigZips/chromFa.tar.gz

#### HG002

- Tier1 benchmark SV callset and high-confidence HG002 region: https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/analysis/NIST_SVs_Integration_v0.6/

- NHGRI_Illumina300X_AJtrio_novoalign_bam: https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/NIST_HiSeq_HG002_Homogeneity-10953946/NHGRI_Illumina300X_AJtrio_novoalign_bams/HG002.hs37d5.60x.1.bam
- NIST_Illumina_2x250bps/novoalign_bam: https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/NIST_Illumina_2x250bps/novoalign_bams/HG002.hs37d5.2x250.bam 
- NIST_BGIseq_2x150bp_100x_bam: https://ftp.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/NIST_BGIseq_2x150bp_100x/GRCh37/HG002_GRCh37_BGIseq-2x150-100x_NIST_20211126.bam 

## Models that have been trained

| File name            | URL                                                          |
| -------------------- | ------------------------------------------------------------ |
| MobileNet_Attention  | https://github.com/nudt-bioinfo/SVScope/releases/download/v1.0.0/MobileNet_Attention.ckpt |
| ShuffleNet_Attention | https://github.com/nudt-bioinfo/SVScope/releases/download/v1.0.0/ShuffleNet_Attention.ckpt |
| ResNet34_Attention   | https://github.com/nudt-bioinfo/SVScope/releases/download/v1.0.0/ResNet34_Attention.ckpt |
| ResNet50_Attention   | https://github.com/nudt-bioinfo/SVScope/releases/download/v1.0.0/ResNet50_Attention.ckpt |

## Usage

### Train

In the src file

VCF data preprocessing:

```
python vcf_data_process.py
```

BAM data preprocessing:

```
python bam2depth.py
```

Parallel generate images:

```
python parallel_process_file.py --thread_num thread_num  
(python parallel_process_file.py --thread_num 8)
```

Check generated images:

```
python process_file_check.py
```

Rearrange generated images:

```
python data_spread.py
```

Train:

```
python train.py
```

### Predict & Filter

predict:

```
python predict.py selected_model
(e.g. python predict.py resnet50_Attention)
```

filter:

```
python filter.py selected_model
(e.g. python filter.py resnet50_Attention)
```
